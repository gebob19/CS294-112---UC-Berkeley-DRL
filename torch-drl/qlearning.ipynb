{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = env.observation_space.shape[0]\n",
    "dim_out = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dim_in, H, dim_out):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_in, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, H),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(H, dim_out)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer(object):\n",
    "    def __init__(self, size_limit):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.size = 0\n",
    "        self.size_limit = size_limit\n",
    "        self.dtypes = [np.float32, np.int64, np.float32, np.float32, np.float32]\n",
    "    \n",
    "    def add(self, s, a, r, ns, done):\n",
    "        self.states.append(s)\n",
    "        self.actions.append(a)\n",
    "        self.rewards.append(r)\n",
    "        self.next_states.append(ns)\n",
    "        self.dones.append(done)\n",
    "        self.size += 1\n",
    "        \n",
    "        if self.size > self.size_limit:\n",
    "            self.states.pop(0)\n",
    "            self.actions.pop(0)\n",
    "            self.rewards.pop(0)\n",
    "            self.next_states.pop(0)\n",
    "            self.dones.pop(0)\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        # get batch indexes\n",
    "        indexs = np.arange(self.size)\n",
    "        np.random.shuffle(indexs)\n",
    "        batch_i = indexs[:batch_size]\n",
    "        \n",
    "        batch = []\n",
    "        # extract batch indexes\n",
    "        for dset, dtype in zip([self.states, self.actions, self.rewards, self.next_states, self.dones], self.dtypes):\n",
    "            batch.append(np.asarray(dset, dtype=dtype)[batch_i])\n",
    "\n",
    "        # convert to tensors\n",
    "        for i, dset in enumerate(batch):\n",
    "            batch[i] = torch.tensor(dset)\n",
    "        \n",
    "        return tuple(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_env(buffer, Qfunc, num_samples, e):\n",
    "    obs = env.reset()\n",
    "    count = 0\n",
    "    rewards = 0\n",
    "    while count < num_samples:\n",
    "        # start with random sample policy\n",
    "        if np.random.randint(0, high=1) < e:\n",
    "            act = env.action_space.sample()\n",
    "        else:\n",
    "            act = Qfunc(torch.tensor(np.asarray(obs, dtype=np.float32)))\n",
    "            act = torch.argmax(act).item()\n",
    "            \n",
    "        next_obs, reward, done, _ = env.step(act)\n",
    "        rewards += reward\n",
    "        buffer.add(obs, act, reward, next_obs, done)\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        obs = next_obs\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Qtarget, Qfunc, buffer, num_samples, num_qtarg_update, num_resample, num_q_update, batch_size=1000, gamma=torch.tensor(0.99, dtype=torch.float32)):\n",
    "    loss_fn = nn.L1Loss()\n",
    "    optim = torch.optim.Adam(Qfunc.parameters(), lr=1e-2)\n",
    "    e = 0.9\n",
    "    \n",
    "    for i in range(num_qtarg_update):\n",
    "        # update Q'(s, a) = Q(s, a)\n",
    "        Qtarget.load_state_dict(Qfunc.state_dict())\n",
    "        \n",
    "        # sample more data to buffer\n",
    "        for _ in range(num_resample):\n",
    "            # sample from env \n",
    "            while buffer.size < batch_size:\n",
    "                sample_env(buffer, Qfunc, num_samples, e ** i)\n",
    "            # update Q(s, a) with k steps\n",
    "            for _ in range(num_q_update):\n",
    "                obs_k, act_k, rew_k, obs_n_k, done_k = buffer.sample_batch(batch_size) \n",
    "                # Q(s, a) of taken action\n",
    "                qpred_k = Qfunc(obs_k)\n",
    "                qpred_k = qpred_k[np.arange(len(qpred_k)), act_k]\n",
    "                \n",
    "                # Double Q Learning\n",
    "                # y = r(s, a) + gamma * Q'(s', max_a' Q(s', a'))\n",
    "                qtarget_pred = Qtarget(obs_n_k)\n",
    "                qfunc_act = torch.argmax(Qfunc(obs_n_k), 1)\n",
    "                qtarget_pred = qtarget_pred[np.arange(len(qtarget_pred)), qfunc_act]\n",
    "                qtarget_pred = qtarget_pred.detach()\n",
    "                \n",
    "                done_mask = torch.ones_like(done_k) - done_k\n",
    "                y_k = rew_k + done_mask * gamma * qtarget_pred\n",
    "\n",
    "                loss = (qpred_k - y_k).sum()\n",
    "                optim.zero_grad()\n",
    "                \n",
    "                # grad descent\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Qfunc):\n",
    "    obs = env.reset()\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        act = Qfunc(torch.tensor(np.asarray(obs, dtype=np.float32)))\n",
    "        act = torch.argmax(act).item()\n",
    "            \n",
    "        obs, reward, done, _ = env.step(act)\n",
    "        rewards += reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 100\n",
    "Qtarget = build_model(dim_in, H, dim_out)\n",
    "Qfunc = build_model(dim_in, H, dim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2c22a314cad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mnum_resample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mnum_q_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   batch_size=2000)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reward:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5609d4fe8e95>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(Qtarget, Qfunc, buffer, num_samples, num_qtarg_update, num_resample, num_q_update, batch_size, gamma)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# update Q(s, a) with k steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_q_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mobs_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_n_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;31m# Q(s, a) of taken action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mqpred_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1d738547badf>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# extract batch indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "buffer = replay_buffer(size_limit=1000000)\n",
    "e_loss = []\n",
    "for e in range(epoch):\n",
    "    print('Epoch:', e)\n",
    "    loss = train(Qtarget, Qfunc, buffer,\n",
    "                  num_samples=5000,\n",
    "                  num_qtarg_update=5000,\n",
    "                  num_resample=10000, \n",
    "                  num_q_update=4,\n",
    "                  batch_size=2000)\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Reward:\", test(Qfunc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Qfunc.state_dict(), './weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!python run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
