{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.lin_in = torch.nn.Linear(D_in, H)\n",
    "        self.lin_hd = torch.nn.Linear(H, H)\n",
    "        self.lin_out = torch.nn.Linear(H, D_out)\n",
    "        self.num_hidden = num_hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lin_in(x).clamp(min=0)\n",
    "        for _ in range(self.num_hidden):\n",
    "            x = self.lin_hd(x).clamp(min=0)\n",
    "        y = self.lin_out(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rollout_replay():\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.dtypes = [torch.float32, torch.int64, torch.float32, torch.float32, torch.float32]\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward) \n",
    "        self.next_states.append(next_state) \n",
    "        self.dones.append(done)    \n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        indexes = np.arange(len(self.states))\n",
    "        np.random.shuffle(indexes)\n",
    "        batch_indexes = indexes[:batch_size]\n",
    "        \n",
    "        # batch and convert to tensors\n",
    "        data = []\n",
    "        for buf, dtype in zip([self.states,  self.actions, self.rewards, self.next_states, self.dones], self.dtypes):\n",
    "            batch = torch.tensor(buf[batch_indexes], dtype=dtype)\n",
    "            data.append(batch)\n",
    "        return tuple(data)\n",
    "\n",
    "    def tensors(self):\n",
    "        # convert to tensors\n",
    "        data = []\n",
    "        for buf, dtype in zip([self.states,  self.actions, self.rewards, self.next_states, self.dones], self.dtypes):\n",
    "            data.append(torch.tensor(buf, dtype=dtype))\n",
    "        return tuple(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(N):\n",
    "    replay = rollout_replay()\n",
    "    \n",
    "    for _ in range(N):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Stochastic Sampling\n",
    "            act = env.action_space.sample()\n",
    "            next_state, rew, done, _ = env.step(act)\n",
    "            replay.add(state, act, rew, next_state, done)\n",
    "            state = next_state\n",
    "    \n",
    "    return replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_VFunc(replay_buff):\n",
    "    gamma = 0.99\n",
    "    loss_fcn = torch.nn.MSELoss()\n",
    "    optim = torch.optim.Adam(VFunc.parameters(), lr=1e-2)\n",
    "    \n",
    "    states, _, rewards, next_states, dones = replay_buff.tensors()\n",
    "\n",
    "    state_val = rewards + (1 - dones) * gamma * torch.squeeze(VFunc(next_states))\n",
    "    state_val = state_val.detach()\n",
    "    \n",
    "    state_val_pred = torch.squeeze(VFunc(states))\n",
    "    \n",
    "    loss = loss_fcn(state_val_pred, state_val)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return replay_buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv(rewards, states, next_states, dones):\n",
    "    v_s = torch.squeeze(VFunc(states))\n",
    "    v_ns = torch.squeeze(VFunc(next_states))\n",
    "    return rewards + (1 - dones) * v_ns - v_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logprob(states, actions):\n",
    "    act_distrib = QFunc(states)\n",
    "    act_prob = torch.nn.LogSoftmax(dim=1)(act_distrib)\n",
    "    return act_prob[np.arange(len(act_prob)), actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_rollouts):\n",
    "    replay_buff = rollout(num_rollouts)\n",
    "    buff = fit_VFunc(replay_buff)\n",
    "\n",
    "    states, actions, rewards, next_states, dones = buff.tensors()\n",
    "    adv = get_adv(rewards, states, next_states, dones)\n",
    "    adv = adv.detach()\n",
    "    logprob = get_logprob(states, actions)\n",
    "\n",
    "    optim = torch.optim.Adam(QFunc.parameters(), lr=1e-2)\n",
    "    loss = -1 * (logprob * adv).mean()\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_greedy(render=False):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if render: \n",
    "            plt.imshow(env.render(mode='rgb_array'))\n",
    "            display.display(plt.gcf())    \n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "        act = torch.argmax(QFunc(torch.tensor(state, dtype=torch.float32)), dim=0)\n",
    "        state, reward, done, _ = env.step(act.item())\n",
    "        total_reward += reward\n",
    "        \n",
    "    print(\"TotalReward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, N = 64, 4 \n",
    "# Two Network Design\n",
    "VFunc = SimpleNet(D_in=state_dim, D_out=1, H=H, num_hidden=N)\n",
    "QFunc = SimpleNet(D_in=state_dim, D_out=action_dim, H=H, num_hidden=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "N = 500\n",
    "for e in range(epochs):\n",
    "    train(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalReward: 56.0\n"
     ]
    }
   ],
   "source": [
    "view_greedy(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
